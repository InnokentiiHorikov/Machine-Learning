{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8253588,"sourceType":"datasetVersion","datasetId":4897679}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import softmax\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# ГПУ для більших швидких обчислень\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:48.835881Z","iopub.execute_input":"2024-05-24T09:30:48.836255Z","iopub.status.idle":"2024-05-24T09:30:48.841604Z","shell.execute_reply.started":"2024-05-24T09:30:48.836226Z","shell.execute_reply":"2024-05-24T09:30:48.840588Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/sarcasm-detection/sarcasm_detection.csv\")\ndata = data[['headline', 'is_sarcastic']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:48.844718Z","iopub.execute_input":"2024-05-24T09:30:48.845083Z","iopub.status.idle":"2024-05-24T09:30:48.987797Z","shell.execute_reply.started":"2024-05-24T09:30:48.845048Z","shell.execute_reply":"2024-05-24T09:30:48.986848Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"                                            headline  is_sarcastic\n0  thirtysomething scientists unveil doomsday clo...             1\n1  dem rep. totally nails why congress is falling...             0\n2  eat your veggies: 9 deliciously different recipes             0\n3  inclement weather prevents liar from getting t...             1\n4  mother comes pretty close to using word 'strea...             1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>is_sarcastic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>thirtysomething scientists unveil doomsday clo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dem rep. totally nails why congress is falling...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eat your veggies: 9 deliciously different recipes</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>inclement weather prevents liar from getting t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mother comes pretty close to using word 'strea...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['is_sarcastic'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:48.989559Z","iopub.execute_input":"2024-05-24T09:30:48.989895Z","iopub.status.idle":"2024-05-24T09:30:48.998167Z","shell.execute_reply.started":"2024-05-24T09:30:48.989868Z","shell.execute_reply":"2024-05-24T09:30:48.997168Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"is_sarcastic\n0    0.541679\n1    0.458321\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"Бачимо, що значного дізбалансу в даних нема. Тому можемо не добавляти ваги для класів","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split, Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport re\nbatch_size = 32\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\nclass SarcasmDataset(Dataset):\n    '''\n    Класс сімейства Датасет для наших даних \n    '''\n    def __init__(self, data, transform = None):\n        '''\n        Конструктор\n        '''\n        self.data = data\n        self.transform = transform\n        \n        \n    def __len__(self):\n        return len(self.data)\n\n    \n    def __getitem__(self, idx):\n\n        item = self.data.iloc[idx]\n        \n        target = int(item['is_sarcastic'])\n        sentence = item.headline\n        'Токенайзуємо речення та екстрактимо токени зі масками'\n        tokens = tokenizer.encode_plus(\n            sentence,\n            add_special_tokens = True,\n            max_length=64,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n\n        seq = tokens['input_ids']\n        mask = tokens['attention_mask']\n        \n        if self.transform:\n            seq = self.transform(seq)\n        \n        'Повертаємо токени зі масками речення, та класс, до якого належить оброблене речення'\n        return {\n            'input_ids': seq,\n            'attention_mask': mask,\n            'class': target\n         }\n    \ndataset = SarcasmDataset(data) #Ініцюалізуємо\ntrainset, testset = random_split(dataset, [0.7, 0.3]) #Ділимо на train та test\n#Та ініцюалізуємо train та test-дані \ntrain_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(testset, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:48.999358Z","iopub.execute_input":"2024-05-24T09:30:48.999649Z","iopub.status.idle":"2024-05-24T09:30:49.154347Z","shell.execute_reply.started":"2024-05-24T09:30:48.999624Z","shell.execute_reply":"2024-05-24T09:30:49.153472Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self):\n        super(BERT_Arch, self).__init__()\n        \n        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n\n        \n        self.fc1 = nn.Sequential(nn.Dropout(0.1),\n                                nn.Linear(768,256),\n                                nn.ReLU())\n      \n        self.fc2 = nn.Sequential(nn.Dropout(0.1),\n                                nn.Linear(256,64),\n                                nn.ReLU())\n\n        self.fc3 = nn.Sequential(nn.Dropout(0.1),\n                                nn.Linear(64,1))\n    #define the forward pass\n    def forward(self, sent_id, mask):\n        \n        #Берт Модель \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n        #І поверх неї модель класифікації\n\n        x = self.fc1(cls_hs)\n        \n        x = self.fc2(x)\n        \n        # output layer\n        x = self.fc3(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:49.156515Z","iopub.execute_input":"2024-05-24T09:30:49.156841Z","iopub.status.idle":"2024-05-24T09:30:49.164833Z","shell.execute_reply.started":"2024-05-24T09:30:49.156813Z","shell.execute_reply":"2024-05-24T09:30:49.163925Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"model = BERT_Arch().to(dtype=torch.float16, device='cuda')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:49.166006Z","iopub.execute_input":"2024-05-24T09:30:49.166256Z","iopub.status.idle":"2024-05-24T09:30:49.620272Z","shell.execute_reply.started":"2024-05-24T09:30:49.166232Z","shell.execute_reply":"2024-05-24T09:30:49.619346Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\nimport tqdm\nfrom torchmetrics.classification import BinaryAccuracy\n# define the optimizer\nloss_f = torch.nn.BCEWithLogitsLoss()\noptimizer = AdamW(model.parameters(),lr = 1e-5) \nepochs = 10\naccuracy = BinaryAccuracy().to(device)\nfor i in range(epochs):\n    print(f\"Epoch {i}\")\n    # Train loop\n    train_tqdm = tqdm.tqdm(train_loader)\n    model.train()\n    for batch in train_tqdm:\n        input_ids, attention_mask, cls = batch['input_ids'], batch['attention_mask'], batch['class']\n        \n        # Передаємо до ГПУ наші тензори'\n        input_ids, attention_mask, cls = input_ids.to(device), attention_mask.to(device), cls.to(device)\n        #Онуляємо градієнти \n        optimizer.zero_grad()\n        model.zero_grad()\n        \n        #Предиктимо класи\n        output = model(input_ids.squeeze(1), attention_mask.squeeze(1)).squeeze(1)\n        \n        # Рахуємо трати\n        L = loss_f(output, cls.float())\n        \n        acc = accuracy(softmax(output), cls)\n        #Передаємо до моделі градієнт втрат\n        L.backward()\n        \n        \n        optimizer.step() #Оновлення параметрів отпимізатору\n        train_tqdm.set_description(f\"Train loss: {L.item()}  Accuracy -- {acc}\") #Вивід у термінал значення втрат\n        train_tqdm.refresh()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:30:49.621653Z","iopub.execute_input":"2024-05-24T09:30:49.622292Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1211 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_35/4234795135.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  acc = accuracy(softmax(output), cls)\nTrain loss: 0.5916748046875  Accuracy -- 0.800000011920929: 100%|██████████| 1211/1211 [03:50<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Train loss: 0.6448150873184204  Accuracy -- 0.800000011920929: 100%|██████████| 1211/1211 [03:51<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"Train loss: 0.6726970672607422  Accuracy -- 0.5:   6%|▌         | 74/1211 [00:14<03:37,  5.24it/s]    ","output_type":"stream"}]}]}